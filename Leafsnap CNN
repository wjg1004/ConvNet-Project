### Packages ###

import sys
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.optimizers import SGD,RMSprop,adam
from keras.utils import np_utils

import numpy as np
import matplotlib.pyplot as plt
import matplotlib
import tensorflow
from PIL import Image
from numpy import *
from scipy.misc import imread

from sklearn.utils import shuffle
from sklearn.cross_validation import train_test_split
from sklearn.metrics import classification_report,confusion_matrix

import skimage
from skimage import io
import os

### Specify Paths ###

new_folder = 'C:\\Users\\Berg\\Desktop\\all_segmented_leaves' # specify an existing folder to store the leaves
lab_leaves_path = 'C:\\Users\\Berg\\Desktop\\lab' # specify lab leaves path
field_leaves_path = 'C:\\Users\\Berg\\Desktop\\field' # specify field leaves path

### Get Leaf Names ###

list_leaves_lab = os.listdir(lab_leaves_path)
list_leaves_field = os.listdir(field_leaves_path)

# WARNING: Creates 20 MB of data, only run during first run through #
# Organize data #

##### resize and put all lab-segmented images into one file #####

for leaf_folder in list_leaves_lab:
    temp_list = os.listdir(lab_leaves_path + '\\' + leaf_folder)
    for leaf_image in temp_list:
        im = Image.open(lab_leaves_path + '\\' + leaf_folder + '\\' + leaf_image)
        img = im.resize((64,64)) # reference users manual
        img.save(new_folder + '\\' + leaf_image, "JPEG")        

        
##### resize and put all field-segmented images into one file #####

for leaf_folder in list_leaves_field:
    temp_list = os.listdir(field_leaves_path + '\\' + leaf_folder)
    for leaf_image in temp_list:
        im = Image.open(field_leaves_path + '\\' + leaf_folder + '\\' + leaf_image)
        img = im.resize((64,64)) # reference users manual
        img.save(new_folder + '\\' + leaf_image, "JPEG")     

### create numeric labels ###

numeric_labels = []
species_labels = []
count = -1

for leaf_folder in list_leaves_lab:
    count = count+1
    temp_list = os.listdir(lab_leaves_path + '\\' + leaf_folder)
    for i in range(0,size(temp_list)):
        numeric_labels.append(count)
        species_labels.append(leaf_folder)
   
count = -1
for leaf_folder in list_leaves_field:
    count = count+1
    temp_list = os.listdir(field_leaves_path + '\\' + leaf_folder)
    for i in range(0,size(temp_list)):
        numeric_labels.append(count)
        species_labels.append(leaf_folder)

# WARNING: >10 minute run time #

##### flatten images and fit their pixels into one array #####

imlist = os.listdir(new_folder)
immatrix = array([((imread(new_folder + '\\' + imlist[0]))).flatten()])
shape(immatrix)
del imlist[0]
for image in imlist:
        immatrix = vstack((immatrix,array([(imread(new_folder + '\\' + image)).flatten()])))

##### shuffle data #####

data,Label = shuffle(immatrix, numeric_labels, random_state=2)
train_data = [data,Label]

##### split data into testing and training sets #####

img_rows, img_cols = 64, 64

(X, y) = (train_data[0],train_data[1])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)

X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)
X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)

X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

# standardize
X_train /= 255
X_test /= 255

###### use variable viewer for more information, the ones are placed to indicate a leaf's species ######
###### rows are individual leaf images, and columns are leaf species #####

nb_classes = 185 

Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)

i = 100
plt.imshow(X_train[i, 0], interpolation='nearest')
print("label : ", Y_train[1,:])

print('X_train shape:', X_train.shape)
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')

############# Setting Model Parameters ################

#batch_size to train
batch_size = 32
# number of epochs to train
nb_epoch = 1


# number of convolutional filters to use
nb_filters = 32
# size of pooling area for max pooling
nb_pool = 2
# convolution kernel size
nb_conv = 3

############ Model initialization #####################

model = Sequential()

model.add(Convolution2D(nb_filters, kernel_size=(nb_conv),
                        input_shape=(img_rows, img_cols, 1),
                        padding='same'))
convout1 = Activation('relu')
model.add(convout1)
model.add(Convolution2D(nb_filters, nb_conv, nb_conv))
convout2 = Activation('relu')
model.add(convout2)
model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))
model.add(Dropout(0.5))

model.add(Flatten())
model.add(Dense(128))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes))
model.add(Activation('softmax'))
model.compile(loss='sparse_categorical_crossentropy',optimizer = 'adam',metrics=["accuracy"])

## train the model ###

hist_fit=model.fit(X_train,y_train,batch_size=batch_size,epochs=nb_epoch,verbose=1, validation_data=(X_test,y_test))

###### visualizing losses and accuracy #####

train_loss=hist_fit.history['loss']
val_loss=hist_fit.history['val_loss']
train_acc=hist_fit.history['acc']
val_acc=hist_fit.history['val_acc']
xc=range(nb_epoch)

plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss)
plt.plot(xc,val_loss)
plt.xlabel('num of Epochs')
plt.ylabel('loss')
plt.title('train_loss vs val_loss')
plt.grid(True)
plt.legend(['train','val'])
print(plt.style.available) # use bmh, classic,ggplot for big pictures
plt.style.use(['classic'])

plt.figure(2,figsize=(7,5))
plt.plot(xc,train_acc)
plt.plot(xc,val_acc)
plt.xlabel('num of Epochs')
plt.ylabel('accuracy')
plt.title('train_acc vs val_acc')
plt.grid(True)
plt.legend(['train','val'],loc=4)
#print plt.style.available # use bmh, classic,ggplot for big pictures
plt.style.use(['classic'])

### Confusion Matrix ###

prob_predictions = model.predict(X_test)
class_predictions = np.argmax(prob_predictions, axis=1)
print(prob_predictions)
print(class_predictions)

metric_report = classification_report(np.argmax(Y_test,axis=1), class_predictions,target_names=list_leaves_lab)
cf_matrix = confusion_matrix(np.argmax(Y_test,axis=1), class_predictions)
                                      
print(metric_report)
print(cf_matrix)

### Top 3 Probable Species ###

pred        = prob_predictions
top_prob    = np.empty([len(pred),3])
top_indices = np.empty([len(pred),3])
row_max     = 0
c           = 0
 
for k in range(3):
    for i in range(len(pred)):
        row_max          = np.max(pred[i]) #stores max value for row
        c                = np.argmax(pred[i]) #stores max value col index
        top_indices[i,k] = c
        top_prob[i,k]    = row_max #stores max in row
        pred[i,c]        = 0 #replaces max value with 0 

# Accuracy with Top 3 species #

correct = 0

for i in range(len(pred)):
    for k in range(3):
        if top_indices[i,k] == y_test[i]:
            correct = correct + 1
            
print(correct)
new_acc = correct / len(pred)
print(new_acc)

# Conditional Probabilites #
# Given that it is one of the top 3 species, what is the probability of it being correct
cd_prob   = np.empty([len(pred),3])
sum_prob = 0

for i in range(len(pred)):
    sum_prob = top_prob[i].sum() #sums ith row 
    for k in range(3):
        cd_prob[i,k] = top_prob[i,k]/sum_prob 

