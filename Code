#-------------------------------------------------------------------------------
# Name:          Simple Sequential Model - or a linear stack
# Purpose:       Demonstrates the use of a multi-layer perceptron
#                to classify wine as either red or white
#
# Last Edit:     02/19/2018
# Condition:     Model evaluation may not work because of data type of predicted
#                 values
#-------------------------------------------------------------------------------

### Importing and checking data ###

import pandas as pd

# Reading in white and red wine data
red = pd.read_csv("http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv", sep=';')
white = pd.read_csv("http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv", sep=';')

# Printing first few lines of red and white winedata
print(red.head())
print(white.head())

# Checking for null values
print(pd.isnull(red))
print(pd.isnull(white))

# Adding `type` column to create classes
red['type'] = 1
white['type'] = 0

# Appending `white` to `red`
wines = red.append(white, ignore_index=True)

### Train and Test splitting ###

import numpy as np
from sklearn.model_selection import train_test_split

# Specify the data
X=wines.ix[:,0:11]

# Specify the target labels and flatten the array
y=np.ravel(wines.type)

# Split the data up in train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

### Standardization ###

# Import `StandardScaler` from `sklearn.preprocessing`
from sklearn.preprocessing import StandardScaler

# Define the scaler
scaler = StandardScaler().fit(X_train)

# Scale the train and test sets
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

### Create the sequential model and adding layers ###

from keras.models import Sequential
from keras.layers import Dense

# Initialize the constructor
model = Sequential()

# Add an input layer: output will contain 12 hidden units which is the amount
# of 'freedom' the model will have when interpreting features
model.add(Dense(12, activation='relu', input_shape=(11,)))

# Add one hidden layer
model.add(Dense(8, activation='relu'))

# Add an output layer
model.add(Dense(1, activation='sigmoid'))

### Explore the Model ###

# Model output shape
model.output_shape

# Model summary
model.summary()

# Model config
model.get_config()

# List all weight tensors
model.get_weights()

### Compiling and fitting/training the model ###

# loss function is binary cross-entropy because right now we're classifying red or white
#       categorical_crossentropy would be used for more than two
# optimization algorithm is Adam (adaptive moment estimation) because it's easy to configure
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# one epoch consists of one run through of the training set and one verification on the test set
# batch size is the number of samples being run through at one time, keep low to keep efficiency
# verbose=1 displays the progress bars
model.fit(X_train, y_train,epochs=20, batch_size=1, verbose=1)

### predicting values ###

y_pred = model.predict(X_test)
y_pred[:5]  # outputs floats instead of desired binaries which messes up the model evaluation
y_test[:5]

print(y_test[:5])

### evaluating the model ###
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score

score = model.evaluate(X_test, y_test,verbose=1)
print(score)

confusion_matrix(y_test, y_pred)
precision_score(y_test, y_pred)
recall_score(y_test, y_pred)
f1_score(y_test,y_pred)
cohen_kappa_score(y_test, y_pred)
