import sys
sys.path.append('c:\\users\\berg\\appdata\\local\\programs\\python\\python36\\lib\\site-packages')

from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.optimizers import SGD,RMSprop,adam
from keras.utils import np_utils

import numpy as np
import matplotlib.pyplot as plt
import matplotlib
import os
import tensorflow
from PIL import Image
from numpy import *
from scipy.misc import imread

from sklearn.utils import shuffle
from sklearn.cross_validation import train_test_split

import skimage
from skimage import io
import os



##### resize and put all images into one file and create a 1-dimensional array for the corresponding labels #####

new_folder = 'C:\\Users\\Berg\\Desktop\\segmented_lab_resized' 
all_leaves_path = 'C:\\Users\\Berg\\Desktop\\lab'
list_leaves = os.listdir(all_leaves_path)
num_leaf_types = size(list_leaves)
labels = []
count=-1

for leaf_folder in list_leaves:
    count = count+1
    temp_list = os.listdir(all_leaves_path + '\\' + leaf_folder)
    for i in range(0,size(temp_list)):
        labels.append(count)
    for leaf_image in temp_list:
        im = Image.open(all_leaves_path + '\\' + leaf_folder + '\\' + leaf_image)
        img = im.resize((64,64))
        img.save(new_folder + '\\' + leaf_image, "JPEG")        


##### flatten the images and fit all of them into one array #####

imlist = os.listdir(new_folder)
immatrix = array([((imread(new_folder + '\\' + imlist[0]))).flatten()])
shape(immatrix)
del imlist[0]
for image in imlist:
        immatrix = vstack((immatrix,array([(imread(new_folder + '\\' + image)).flatten()])))


##### shuffle #####
        
data,Label = shuffle(immatrix, labels, random_state=2)
train_data = [data,Label]


##### split data into testing and training sets #####

img_rows, img_cols = 64, 64

(X, y) = (train_data[0],train_data[1])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)

X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)
X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)

X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

X_train /= 255
X_test /= 255

print('X_train shape:', X_train.shape)
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')


###### create the label matrix where each row is binary for a given leaf ######
nb_classes = 185 

Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)

i = 100
plt.imshow(X_train[i, 0], interpolation='nearest')
print("label : ", Y_train[i,:])


############# Setting Model Parameters ################
n=100
b=25
iteration=4
epoch =1 
#batch_size to train
batch_size = 32
# number of epochs to train
nb_epoch = 50


# number of convolutional filters to use
nb_filters = 32
# size of pooling area for max pooling
nb_pool = 2
# convolution kernel size
nb_conv = 3


############ Model #####################

model = Sequential()

model.add(Convolution2D(nb_filters, kernel_size=(nb_conv),
                        input_shape=(img_rows, img_cols, 1),
                        padding='same'))
convout1 = Activation('relu')
model.add(convout1)
model.add(Convolution2D(nb_filters, nb_conv, nb_conv))
convout2 = Activation('relu')
model.add(convout2)
model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))
model.add(Dropout(0.5))

model.add(Flatten())
model.add(Dense(128))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes))
model.add(Activation('softmax'))
model.compile(loss='sparse_categorical_crossentropy',optimizer = 'adam',metrics=["accuracy"])


hist_fit=model.fit(X_train,y_train,batch_size=batch_size,epochs=nb_epoch,verbose=1, validation_data=(X_test,y_test))

##################### visualizing losses and accuracy ############################

train_loss=hist_fit.history['loss']
val_loss=hist_fit.history['val_loss']
train_acc=hist_fit.history['acc']
val_acc=hist_fit.history['val_acc']
xc=range(nb_epoch)

plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss)
plt.plot(xc,val_loss)
plt.xlabel('num of Epochs')
plt.ylabel('loss')
plt.title('train_loss vs val_loss')
plt.grid(True)
plt.legend(['train','val'])
print(plt.style.available) # use bmh, classic,ggplot for big pictures
plt.style.use(['classic'])

plt.figure(2,figsize=(7,5))
plt.plot(xc,train_acc)
plt.plot(xc,val_acc)
plt.xlabel('num of Epochs')
plt.ylabel('accuracy')
plt.title('train_acc vs val_acc')
plt.grid(True)
plt.legend(['train','val'],loc=4)
#print plt.style.available # use bmh, classic,ggplot for big pictures
plt.style.use(['classic'])

### Confusion Matrix ###
from sklearn.metrics import classification_report,confusion_matrix
prob_predictions = model.predict(X_test)
class_predictions = np.argmax(prob_predictions, axis=1)
print(prob_predictions)
print(class_predictions)

metric_report = classification_report(np.argmax(Y_test,axis=1), class_predictions,target_names=list_leaves)
cf_matrix = confusion_matrix(np.argmax(Y_test,axis=1), class_predictions)
                                      
print(metric_report)
print(cf_matrix)

## saving & loading weights ##
fname = "weights-Warren_Edit_4-23-18.hdf5"
#model.save_weights(fname,overwrite=True)

model.load_weights(fname)



## Parker's Stuff ##
predictions = np.transpose(predictions)
pd.da(predictions)
predictions.insert(0, 'New_ID', range(1, 1 + len(predictions)))
print(confusion_matrix(np.argmax(Y_test,axis=1), y_pred))





##########################
### segmented Field data ###
##########################

##### resize and put all images into one file and create a 1-dimensional array for the corresponding labels #####

new_folder = 'C:\\Users\\Berg\\Desktop\\segmented_field_resized' 
all_leaves_path = 'C:\\Users\\Berg\\Desktop\\field'
list_leaves = os.listdir(all_leaves_path)
num_leaf_types = size(list_leaves)
labels = []
real_labels = []
count=-1

for leaf_folder in list_leaves:
    count = count+1
    temp_list = os.listdir(all_leaves_path + '\\' + leaf_folder)
    for i in range(0,size(temp_list)):
        labels.append(count)
        real_labels.append(leaf_folder)
#    for leaf_image in temp_list:
#        im = Image.open(all_leaves_path + '\\' + leaf_folder + '\\' + leaf_image)
#       img = im.resize((64,64))
#        img.save(new_folder + '\\' + leaf_image, "JPEG")
                


##### flatten the images and fit all of them into one array #####

imlist = os.listdir(new_folder)
immatrix = array([((imread(new_folder + '\\' + imlist[0]))).flatten()])
shape(immatrix)
del imlist[0]
for image in imlist:
        immatrix = vstack((immatrix,array([(imread(new_folder + '\\' + image)).flatten()])))


##### shuffle #####
        
data,Label = shuffle(immatrix, labels, random_state=2)
train_data = [data,Label]


##### split data into testing and training sets #####

img_rows, img_cols = 64, 64

(X, y) = (train_data[0],train_data[1])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)

X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)
X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)

X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

X_train /= 255
X_test /= 255

print('X_train shape:', X_train.shape)
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')


###### create the label matrix where each row is binary for a given leaf ######
nb_classes = 185 

Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)

i = 100
plt.imshow(X_train[i, 0], interpolation='nearest')
print("label : ", Y_train[i,:])

############# real Label array #############

import pandas as pd
from scipy.stats import itemfreq

(predictions == y_test).sum()
82/1544
