# -*- coding: utf-8 -*-
"""
Created on Wed Apr 25 19:02:56 2018

@author: Berg
"""

import sys
sys.path.append('c:\\users\\berg\\appdata\\local\\programs\\python\\python36\\lib\\site-packages')
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.optimizers import SGD,RMSprop,adam
from keras.utils import np_utils

import numpy as np
import matplotlib.pyplot as plt
import matplotlib
import os
import tensorflow
from PIL import Image
from numpy import *
from scipy.misc import imread

from sklearn.utils import shuffle
from sklearn.cross_validation import train_test_split

import skimage
from skimage import io
import os



##### resize and put all images into one file and create a 1-dimensional array for the corresponding labels #####

new_folder = 'C:\\Users\\Berg\\Desktop\\segmented_lab_resized' 
all_leaves_path = 'C:\\Users\\Berg\\Desktop\\lab'
list_leaves = os.listdir(all_leaves_path)
num_leaf_types = size(list_leaves)
labels = []
count=-1

for leaf_folder in list_leaves: #counts number of leaf species
    count = count+1
    temp_list = os.listdir(all_leaves_path + '\\' + leaf_folder)
    for i in range(0,size(temp_list)): #fills the label array with numeric value
        labels.append(count)
    for leaf_image in temp_list: #resizing and aggregating images
        im = Image.open(all_leaves_path + '\\' + leaf_folder + '\\' + leaf_image)
        img = im.resize((64,64))
        img.save(new_folder + '\\' + leaf_image, "JPEG")        


##### flatten the images and fit all of them into one array #####

imlist = os.listdir(new_folder)
immatrix = array([((imread(new_folder + '\\' + imlist[0]))).flatten()])
shape(immatrix)
del imlist[0]
for image in imlist:
        immatrix = vstack((immatrix,array([(imread(new_folder + '\\' + image)).flatten()])))


file = open('C:\\Users\\Berg\\Desktop\\CNN_Model\\flattened.txt','w')
file.write(str(immatrix))

##### shuffle #####
        
data,Label = shuffle(immatrix, labels, random_state=2)
train_data = [data,Label]


##### split data into testing and training sets #####

img_rows, img_cols = 64, 64

(X, y) = (train_data[0],train_data[1])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)

X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)
X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)

X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

X_train /= 255
X_test /= 255

print('X_train shape:', X_train.shape)
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')


###### create the label matrix where each row is binary for a given leaf ######
nb_classes = 185 

Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)

i = 100
plt.imshow(X_train[i, 0], interpolation='nearest')
print("label : ", Y_train[i,:])


############# Setting Model Parameters ################

#batch_size to train
batch_size = 32
# number of epochs to train
nb_epoch = 50
# number of convolutional filters to use
nb_filters = 32
# size of pooling area for max pooling
nb_pool = 2
# convolution kernel size
nb_conv = 3


############ Model #####################

#if loading weights, initialize model first then do model.load_weights()

model = Sequential()

model.add(Convolution2D(nb_filters, kernel_size=(nb_conv),
                        input_shape=(img_rows, img_cols, 1),
                        padding='same'))
convout1 = Activation('relu')
model.add(convout1)
model.add(Convolution2D(nb_filters, nb_conv, nb_conv))
convout2 = Activation('relu')
model.add(convout2)
model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))
model.add(Dropout(0.5))

model.add(Flatten())
model.add(Dense(128))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes))
model.add(Activation('softmax'))
model.compile(loss='sparse_categorical_crossentropy',
              optimizer = 'adam',
              metrics=["accuracy"])

## saving & loading weights ##
#fname = "weights-Warren_Edit_4-23-18.hdf5"
#model.save_weights(fname,overwrite=True) #saves to working directory
#model.load_weights(fname)

#Training the model; does not need run if weights are loaded
hist_fit=model.fit(X_train,y_train,batch_size=batch_size,epochs=nb_epoch,verbose=1, validation_data=(X_test,y_test))

##################### visualizing losses and accuracy ############################

train_loss=hist_fit.history['loss']
val_loss=hist_fit.history['val_loss']
train_acc=hist_fit.history['acc']
val_acc=hist_fit.history['val_acc']
xc=range(nb_epoch)

plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss)
plt.plot(xc,val_loss)
plt.xlabel('num of Epochs')
plt.ylabel('loss')
plt.title('train_loss vs val_loss')
plt.grid(True)
plt.legend(['train','val'])
print(plt.style.available) 
plt.style.use(['classic'])

plt.figure(2,figsize=(7,5))
plt.plot(xc,train_acc)
plt.plot(xc,val_acc)
plt.xlabel('num of Epochs')
plt.ylabel('accuracy')
plt.title('train_acc vs val_acc')
plt.grid(True)
plt.legend(['train','val'],loc=4)
plt.style.use(['classic'])

### Confusion Matrix ###
from sklearn.metrics import classification_report,confusion_matrix
prob_predictions = model.predict(X_test)
class_predictions = np.argmax(prob_predictions, axis=1)
print(prob_predictions)
print(class_predictions)

metric_report = classification_report(np.argmax(Y_test,axis=1), class_predictions,target_names=list_leaves)
cf_matrix = confusion_matrix(np.argmax(Y_test,axis=1), class_predictions)
                                      
print(metric_report)
print(cf_matrix)


### Top 3 Probable Species ###
pred        = prob_predictions
top_prob    = np.empty([len(pred),3])
top_indices = np.empty([len(pred),3])
row_max     = 0
r           = 0
c           = 0
 
for k in range(3):
    for i in range(len(pred)):
        row_max          = np.max(pred[i]) #stores max value for row
        c                = np.argmax(pred[i]) #stores max value col index
        top_indices[i,k] = c
        top_prob[i,k]    = row_max #stores max in row
        pred[i,c]        = 0 #replaces max value with 0 
 

#creating conditional probabilites from top 3 species
#i.e. Given that it is one of the top 3 species, what is the probability of it being correct
cd_prob   = np.empty([len(pred),3])
sum_pronb = 0

for i in range(len(pred)):
    sum_prob = top_prob[i].sum() #sums ith row 
    for k in range(3):
        cd_prob[i,k] = top_prob[i,k]/sum_prob # creates relative probability
        
#new accuracy
class_pred       = class_predictions
t_class_pred     = np.transpose(class_pred)
correct          = 0

for i in range(len(pred)):
    for k in range(3):
        if top_indices[i,k] == t_class_pred[i]:
            correct = correct + 1
print(correct)

4445/4630

new_acc = correct / len(t_class_pred)
print(new_acc)
            
       
##########################
### segmented Field data ###
##########################

##### resize and put all images into one file and create a 1-dimensional array for the corresponding labels #####

new_folder = 'C:\\Users\\Berg\\Desktop\\segmented_field_resized' 
all_leaves_path = 'C:\\Users\\Berg\\Desktop\\field'
list_leaves = os.listdir(all_leaves_path)
num_leaf_types = size(list_leaves)
labels = []
real_labels = []
count=-1

for leaf_folder in list_leaves:
    count = count+1
    temp_list = os.listdir(all_leaves_path + '\\' + leaf_folder)
    for i in range(0,size(temp_list)):
        labels.append(count)
        real_labels.append(leaf_folder)
#    for leaf_image in temp_list:
#        im = Image.open(all_leaves_path + '\\' + leaf_folder + '\\' + leaf_image)
#       img = im.resize((64,64))
#        img.save(new_folder + '\\' + leaf_image, "JPEG")

     
##### flatten the images and fit all of them into one array #####

imlist = os.listdir(new_folder)
immatrix = array([((imread(new_folder + '\\' + imlist[0]))).flatten()])
shape(immatrix)
del imlist[0]
for image in imlist:
        immatrix = vstack((immatrix,array([(imread(new_folder + '\\' + image)).flatten()])))


##### shuffle #####
        
data,Label = shuffle(immatrix, labels, random_state=2)
train_data = [data,Label]


##### split data into testing and training sets #####

img_rows, img_cols = 64, 64

(X, y) = (train_data[0],train_data[1])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)

X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)
X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)

X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

X_train /= 255
X_test /= 255

print('X_train shape:', X_train.shape)
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')


###### create the label matrix where each row is binary for a given leaf ######
nb_classes = 185 

Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)

i = 100
plt.imshow(X_train[i, 0], interpolation='nearest')
print("label : ", Y_train[i,:])

############# real Label array #############

import pandas as pd
from scipy.stats import itemfreq

(predictions == y_test).sum()
